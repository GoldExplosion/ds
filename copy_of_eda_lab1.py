# -*- coding: utf-8 -*-
"""Copy of EDA lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bI23KZS7ZigmTxsUkwvfVxpAnrMGSxVZ

# Exploratory Data Analysis

## Basics

Imports
"""

import pandas as pd
from scipy.stats import zscore
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import seaborn as sns
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from keras import layers
from keras import models
from keras import optimizers
from keras import losses
from keras import regularizers
from keras import metrics

"""read data"""

data = pd.read_csv("/content/drive/MyDrive/CLASSROOM/SEM1/Data Science Lab/lab1,2,3/cardio_train.csv", delimiter=";")

"""## Distribution Analysis"""

data = data.drop(columns=['id'])

data.head()

for i in data.columns:
  print(i, data[i].skew())

data['cardio']

print(type(data))

data.hist()

data.groupby('cardio').hist()

# scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')

"""## Outlier Analysis and Treatment



"""

data2 = data.apply(zscore) > 3

data2

print(data2.shape)
print(data.shape)

final = []
for i in range(70000):
  for j in data.columns:
    if data2.at[i,j]:
      final.append(i)
print("Number of rows dropped = ", end='')
print(len(final))
data = data.drop(final)

data.shape

"""## NULL Value Treatment"""

data.isnull().sum()

"""Since there is no null value, there is no need for treatment

## Correlation analysis and Multicollinearity reduction
"""

plt.figure(figsize=(10,120))

# Generate a mask to onlyshow the bottom triangle
mask = np.triu(np.ones_like(data.corr(), dtype=bool))

# generate heatmap
sns.heatmap(data.corr(), annot=True, mask=mask, vmin=-1, vmax=1)
plt.title('Correlation Coefficient Of Predictors')
plt.show()

"""There is no strong relationship between the features, Therefore there is no need for treatment since multicollinearity is not found.

## Feature Transformation

### Log Transform
"""

# data['age'] = np.log(data['age'])
# sns.displot(data['age'])
# fig = plt.figure()

# data['weight'] = np.log(data['weight'])
# sns.displot(data['weight'])
# fig = plt.figure()

# data['height'] = np.log(data['height'])
# sns.displot(data['height'])
# fig = plt.figure()

"""## Scaling """

data.head()

considered_cols = ['age','weight','height','ap_lo','ap_hi']
for i in considered_cols:
  data[i] = (data[i]-data[i].min())/(data[i].max()-data[i].min())

data.head()

"""## Applying Tensors and Arrays from numpy and tensorflow"""

numData = data.to_numpy()

numData

tensorData = tf.convert_to_tensor(data)

tensorData

data.head()

"""## Binary Classification

## Data Preprocessing

### one-hot encode the values in the column
"""

data

data['gender'] = data['gender'].map({1:'female',2:'male'})
data['cholesterol'] = data['cholesterol'].map({1:'chol_normal',2:'chol_above_normal',3:'chol_well_above_normal'})
data['gluc'] = data['gluc'].map({1:'gluc_normal',2:'gluc_above_normal',3:'gluc_well_above_normal'})
data = pd.get_dummies(data, columns=['gender', 'cholesterol', 'gluc'], prefix='', prefix_sep='')
data.head()

data.head()

"""### Splitting the dataset into train and test"""

data.shape

inputValue = pd.DataFrame(data.iloc[1,:])

inputValue

data = data.drop(1)

data.head()

train = data.sample(frac=0.8, random_state = 0)
test = data.drop(train.index)

# sns.pairplot(train[train.columns])

train.describe().transpose()

train_features = train.copy()
test_features = test.copy()
train_labels = train_features.pop('cardio')
test_labels = test_features.pop('cardio')

# normalizer = tf.keras.layers.Normalization(axis=-1)
# normalizer.adapt(np.array(train_features))



Result = []

print(type(inputValue))
inputValue.T.columns

input1 = inputValue.T.copy()
expectedOutput = input1.pop('cardio')

input1.columns

"""## Neural Network (Deep Learning)

### Model Building
"""

def build_and_compile_model():
  model = keras.Sequential([
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

"""### Training"""

# %%time
# history = dnn_model.fit(
#     train_features,
#     train_labels,
#     validation_split=0.2,
#     verbose=0, epochs=100)

# dnn_model.save('/content/drive/MyDrive/CLASSROOM/SEM1/Data Science Lab/lab 5/dnnmodel2')

"""### Loading the model"""

from tensorflow.keras.models import load_model

dnn_model = load_model('/content/drive/MyDrive/CLASSROOM/SEM1/Data Science Lab/lab 5/dnnmodel2')

"""### Results"""

print("score on test: ",str(1 - dnn_model.evaluate(test_features,test_labels)))
print("score on train: ", str(1-  dnn_model.evaluate(train_features,train_labels)))

Result.append(["Neural Network(Deep Learning)", 1 - dnn_model.evaluate(train_features,train_labels), 1 - dnn_model.evaluate(test_features,test_labels)])

print(Result)

"""## Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB().fit(train_features, train_labels)

print("score on train: "+ str(mnb.score(train_features, train_labels)))
print("score on test: "+ str(mnb.score(test_features, test_labels)))

Result.append(["Naive Bayes", mnb.score(train_features, train_labels), mnb.score(test_features, test_labels)])

mnb.predict(input1)

expectedOutput

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(train_features, train_labels)
print("score on train: "+ str(lr.score(train_features, train_labels)))
print("score on test: "+str(lr.score(test_features, test_labels)))

lr.predict(input1)

Result.append(["Logistic Regression", lr.score(train_features, train_labels), lr.score(test_features, test_labels)])

"""## K-Nearest Neighbours"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=1)
knn.fit(train_features, train_labels)
knnTrainScore = knn.score(train_features, train_labels)
knnTestScore = knn.score(test_features, test_labels)
print("score on train: ", knnTrainScore)
print("score on test: ", knnTestScore)
Result.append(["K-Nearest Neighbours", knnTrainScore, knnTestScore])

knn.predict(input1)

"""## Support Vector Machine"""

from sklearn.svm import LinearSVC

svm = LinearSVC(C=0.0001)
svm.fit(train_features, train_labels)

svmTrainS = svm.score(train_features, train_labels)
svmTestS = svm.score(test_features, test_labels)
print("score on train: ", svmTrainS)
print("score on test: ", svmTestS)
Result.append(["SVM", svmTrainS, svmTestS])

svm.predict(input1)

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=30,  max_depth = 9)
rf.fit(train_features, train_labels)

rfTrain = rf.score(train_features, train_labels)
rfTest = rf.score(test_features, test_labels)
print("score on train: ", rfTrain)
print("score on test: ", rfTest)
Result.append(["Random Forest", rfTrain, rfTest])

rf.predict(input1)

"""## Comparing the Algorithms"""

Result.sort(key = lambda x: -x[2])

for i in Result:
  print(*i, sep="\t")

f = '{0:>29} {1:>4} {2}'

Result.insert(0, ["Algorithm", "Train", "Test"])

for i in Result:
  print(f.format(i[0], str(i[1])[:6], str(i[2])[:6]))